{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dawn-Of-Justice/Dawn-Of-Justice/blob/main/Code_Your_Own_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ifIL4z4yWHX",
        "outputId": "4aff8edb-1298-4dff-a689-a3c2d71c1d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edc3hQNOYebV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "import numpy as np\n",
        "import random\n",
        "from transformers import GPT2Tokenizer,GPT2LMHeadModel,GPT2Model,GPT2Config\n",
        "import torchvision\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from torch.utils.data import TensorDataset\n",
        "import pandas as pd\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "zWIdC4z9YnZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_table(\"Chat_data.txt\")\n",
        "\n",
        "data = df['data'].values\n"
      ],
      "metadata": {
        "id": "lrSrPTDVOInC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHrMncIlOKJs",
        "outputId": "b543e616-408d-4bbc-ff12-393a232beccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Human 1: Hi!', 'Human 2: What is your favorite holiday?',\n",
              "       'Human 1: one where I get to meet lots of different people.',\n",
              "       'Human 2: What was the most number of people you have ever met during a holiday?',\n",
              "       'Human 1: Hard to keep a count. Maybe 25.'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_table(\"csi_chatbot_dat2.txt\")\n",
        "# bios = df['data:'].values\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
        "\n",
        "bios = data\n",
        "\n",
        "input_id = []\n",
        "attn_mask = []\n",
        "\n",
        "# data = encodings_dict = tokenizer('<|startoftext|>'+ bios[0] + '<|endoftext|>', truncation=True, max_length=768, padding=\"max_length\")\n",
        "\n",
        "for i in range(len(bios)):\n",
        "  #print(bios[i])\n",
        "  encodings_dict = tokenizer('<|startoftext|>'+ bios[i] + '<|endoftext|>', truncation=True, max_length=768, padding=\"max_length\")\n",
        "\n",
        "  input_id.append(encodings_dict['input_ids'])\n",
        "  attn_mask.append(encodings_dict['attention_mask'])\n",
        "  # print(len(input_id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4MWFJp1OUhW",
        "outputId": "af3a98a5-0f2b-4626-c001-b0a3044946f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(input_id),len(attn_mask))\n",
        "input_id = torch.tensor(input_id).long()\n",
        "attn_mask = torch.tensor(attn_mask).long()\n",
        "\n",
        "print(input_id.shape,attn_mask.shape)\n",
        "print(input_id.dtype,attn_mask.dtype)\n"
      ],
      "metadata": {
        "id": "AGjd0y9hO4vK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb97a9d-1a07-4062-9ffa-f50c7b12ddcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1494 1494\n",
            "torch.Size([1494, 768]) torch.Size([1494, 768])\n",
            "torch.int64 torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the GPT tokenizer.\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
        "\n",
        "# dataset = GPT2Dataset(df, tokenizer, max_length=768)\n",
        "\n",
        "dataset = TensorDataset(input_id,attn_mask)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            dataset,  # The training samples. \n",
        "            batch_size = 2 # Trains with this batch size.\n",
        "        )\n",
        "data = enumerate(train_dataloader)\n",
        "# encodings_dict = tokenizer('<|startoftext|>'+ sentences[1] + '<|endoftext|>',return_tensors='pt', truncation=True, max_length=10, padding=\"max_length\")\n",
        "# print(encodings_dict['input_ids'])\n",
        "d,d1 = next(data)\n",
        "print(d1[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxVgK0wN32L2",
        "outputId": "dd6c2bc4-cabf-4a3b-a689-e377ea9132e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "total_samples = len(dataset)\n",
        "n_iterations = math.ceil(total_samples/2)\n",
        "print(total_samples,n_iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwL-_h2333zh",
        "outputId": "ab6e6f40-cb3d-4309-c09a-b8c756da786c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1494 747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I'm not really doing anything with the config buheret\n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "# instantiate the model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
        "\n",
        "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
        "# otherwise the tokenizer and model tensors won't match up\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfxb3iDA5tFc",
        "outputId": "09e91d10-a7b2-499b-a0a5-0b31ebf6e226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50259, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1009\n",
        "total_samples = len(dataset)\n",
        "n_iterations = math.ceil(total_samples/3)\n",
        "print(total_samples,n_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOthFvsI4AHB",
        "outputId": "73e41eef-136a-41c0-ec54-0f81126419f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1494 498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epochs = 5\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "\n",
        "# this produces sample output every 100 steps\n",
        "sample_every = 100"
      ],
      "metadata": {
        "id": "K-J7_Gum4EUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate,\n",
        "                  eps = epsilon\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ5fzFMA4HGe",
        "outputId": "4f4e7447-59cc-4b1a-89e2-5b00d6bc0bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup_steps, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "J-lNZfQh4KSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "t1LOzWgG4u0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup_steps, \n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "  \n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        # print(b_labels)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(  b_input_ids,\n",
        "                          labels=b_labels, \n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]  \n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # Get sample every x batches.\n",
        "        if step % 629 == 0 and not step == 0:\n",
        "\n",
        "            # print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss))\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            sample_outputs = model.generate(\n",
        "                                    bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,   \n",
        "                                    top_k=50, \n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95, \n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Average train loss:{avg_train_loss}\")\n",
        "    if avg_train_loss < 0.02:\n",
        "      print(f\"Yay Got there at Epoch:{epoch_i} Step:{step} Average Loss:{avg_train_loss} Loss{loss.item()}\")    \n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgAO5rc24NSD",
        "outputId": "c2b827a5-3b92-430a-a6e6-7a146b8ae7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 100 ========\n",
            "Training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bP-0S3Xe5EW3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}